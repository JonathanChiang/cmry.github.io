---
layout: post
title: (Pretty) Fast $n$-Gram Features with Python
date: 2015-12-02 00:08:59

mast: gram
---

Recently I have been experimenting with (pretty) fast n-gram extraction for
feature space construction. As I have clearly experienced, there are a bunch of
caveats while building your own functions. This blog post gives a *very* short
introduction to $n$-grams, explains their extraction in examples as well as in
code ([skip to](#method-1---a-little-naive,-but-dependency-free)). And finally
gives a very effective and low memory footprint method of extracting them
(shortcut).

## Extracting $n$-Grams

The extraction of $n$-grams is a commonly used bag-of-words method used in
Natural Language Processing (NLP) to represent a collection of documents.
In a very basic example we want to either distinguish or relate sentence $A$,
$B$, and $C$ which are the following:

``` python
A = 'text about stuff'
B = 'stuff about text'
C = 'text about n-grams'
```

The $n$ in $n$-grams typically refers to a *scope*. One can look at it as going
over the sentence with a window of some size. So with a 1-gram (referred to as
a unigram) we partition the text in chunks of size one. So sentence $A$ would
give us `text`, `about`, and `stuff`. If we would put this into a matrix of
gram by sentence, we would get the following for the three sentences:

|             | $A$    | $B$    | $C$    |
| ----------- |:------:|:------:|:------:|
| text        | 1      | 1      | 1      |
| about       | 1      | 1      | 1      |
| stuff       | 1      | 1      | 0      |
| n-grams     | 0      | 0      | 1      |

Looking at the matrix, one can quickly see that despite the *order* of words,
sentence $A$ and $B$ look the same. If we take their vector:

$\vec{A} = [1, 1, 1, 0], \vec{B} = [1, 1, 1, 0]$

They are identical. This is is what the
bag-of-words entails; words are thrown into a bag, scrambled in any order and
counted. So how can we incorporate some structure into this model? Taking word
bi-grams, where $n = 2$ would probably already help our case. Consider:

|               | $A$    | $B$    | $C$    |
| ------------- |:------:|:------:|:------:|
| text about    | 1      | 0      | 1      |
| about stuff   | 1      | 0      | 0      |
| stuff about   | 0      | 1      | 0      |
| about text    | 0      | 1      | 0      |
| about n-grams | 0      | 0      | 1      |

Now the vectors of $A$, $B$ and $C$ are all unique. Moreover, it can actually be
inferred that $A$ and $C$ have some relation, but they both don't relate to $B$.
Done right? Well, not quite. What if we introduce a fourth sentence:

``` python
D = 'n-grams are handy'
```

We construct the matrix again:

|               | $A$  | $B$  | $C$  | $D$  |
| ------------- |:----:|:----:|:----:|:----:|
| text about    | 1    | 0    | 1    | 0    |
| about stuff   | 1    | 0    | 0    | 0    |
| stuff about   | 0    | 1    | 0    | 0    |
| about text    | 0    | 1    | 0    | 0    |
| about n-grams | 0    | 0    | 1    | 0    |
| n-grams are   | 0    | 0    | 0    | 1    |
| are handy     | 0    | 0    | 0    | 1    |

We could say that $D$ relates to $C$, but it doesn't show with this method. With
our uni-gram method it might, but we run into troubles with A and B being
identical again. Instead of words, we might draw more information from the
character combinations instead. Let's take tetri-grams (3-grams), and see:

|              | $A$    |    $B$ |    $C$ |    $D$ |
| ------------ |:------:|:------:|:------:|:------:|
| tex          | 1      | 1      | 1      | 0      |
| ext          | 1      | 1      | 1      | 0      |
| xta          | 1      | 0      | 1      | 0      |
| tab          | 1      | 0      | 1      | 0      |
| abo          | 1      | 1      | 1      | 0      |
| bou          | 1      | 1      | 1      | 0      |
| out          | 1      | 1      | 1      | 0      |
| tst          | 1      | 0      | 0      | 0      |
| uts          | 1      | 0      | 0      | 0      |
| stu          | 1      | 1      | 0      | 0      |
| tuf          | 1      | 1      | 0      | 0      |
| uff          | 1      | 1      | 0      | 0      |
| ffa          | 0      | 1      | 0      | 0      |
| fab          | 0      | 1      | 0      | 0      |
| utt          | 0      | 1      | 0      | 0      |
| tte          | 0      | 1      | 0      | 0      |
| utn          | 0      | 0      | 1      | 0      |
| tn-          | 0      | 0      | 1      | 0      |
| n-g          | 0      | 0      | 1      | 1      |
| -gr          | 0      | 0      | 1      | 1      |
| gra          | 0      | 0      | 1      | 1      |
| ram          | 0      | 0      | 1      | 1      |
| ams          | 0      | 0      | 1      | 1      |
| msa          | 0      | 0      | 0      | 1      |
| sar          | 0      | 0      | 0      | 1      |
| are          | 0      | 0      | 0      | 1      |
| reh          | 0      | 0      | 0      | 1      |
| eha          | 0      | 0      | 0      | 1      |
| han          | 0      | 0      | 0      | 1      |
| and          | 0      | 0      | 0      | 1      |
| ndy          | 0      | 0      | 0      | 1      |

 > Note that I removed spaces to avoid the matrix blowing up even more.

Now, the overlap between the different sentences is more evident, but
with an explosion of the matrix as a result. As can be observed, different
grams vary in usefulness; for sentence
relatedness, tetri-grams might more useful - however, they obfuscate the
actual word usage. The latter can be seen as problematic when the task is
to distinguish certain topics. For example, if we want to measure the
relatedness of sentences $A$ through $D$ to NLP, it's more effective to
represent `n-grams` as a full chunk rather than `['n-g', '-gr', 'gra', etc.]`.
The explosion of the amount of grams per sentence makes it harder for algorithms
to uncover relations; as it would have to rely on a combination of the grams for
information, rather than their single occurrences. Another caveat is the
sparseness of the vectors; the rarer the occurrences, the more zeroes are
present in sentence vectors, the less information a vector provides per bit.
In the above example this is already pretty evident from a very small example,
imagine a corpus of a million documents. How one would go abouts programming
this in an effective manner is the topic of this particular post.

## Method 1 - a Little Naive, But Dependency Free

First we will consider implementing a simple sliding window of $n$ to extract
the grams from a sentence. Such that, given sentence $A$ again, we get the
grams as we saw in the previous example. So something like:

``` python
In [1]: find_ngrams('text about stuff', n=2)
Out[1]: ['text about', 'about stuff']
```

Or even:

``` python
In [1]: find_ngrams('text about stuff', n_list=[1, 2])
Out[1]: ['text', 'about', 'stuff', 'text about', 'about stuff']
```

An effective algorithm as proven by [this post](http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/) is to abuse zip and slicing in python. So:


``` python
def find_ngrams(sentence, n):
    """Magic n-gram function."""
    inp = sentence.split()
    return zip(*[inp[i:] for i in range(n)])

```

The above method will give us a list of tuples of size $n$, extracted in a
sliding window. If we alter the function slightly, we will be able to achieve
the desired result:

``` python
def find_ngrams(sentence, n_list):
    """Magic n-gram function."""
    inp, grams = sentence.split(), []
    for n in n_list:
      grams += [' '.join(x) for x in zip(*[inp[i:] for i in range(n)])]
    return grams

```
Now we should be able to turn sentences into vectors representing the gram
occurrences in a sentence. So that `the the a a thing` would at least yield
`[2, 2, 1]`. This entails incorporating the search function into a neat class
that can fit the known grams and make sure their index in the vector is the
same for all sentences. A major drawback of this approach is that the corpus
needs to be iterated over twice; once for extracting all possible $n$-grams,
and once this is known, another passover to convert all sentences to vectors.
In any case, a very compact class doing exactly this would look something like:

``` python
class Ngrams:

    def __init__(self, n_list):
        self.n_list = n_list
        self.indices = {}

    def fit(self, sentence):
        """Magic n-gram function fits to vector indices."""
        i, inp = len(self.indices)-1, sentence.split()
        for n in self.n_list:
            for x in zip(*[inp[i:] for i in range(n)]):
                if self.indices.get(x) == None:
                    i += 1
                    self.indices.update({x: i})

    def transform(self, sentence):
        """Given a sentence, convert to a gram vector."""
        v, inp = [0] * len(self.indices), sentence.split()
        for n in self.n_list:
            for x in zip(*[inp[i:] for i in range(n)]):
                if self.indices.get(x) != None:
                    v[self.indices[x]] += 1
        return v
```

This works, proven by the example below. Please pay close attention to the
`dict.get == None` or `!= None` parts. Given that we have a `{gram: index}`
dictionary, a simple `if self.indices.get` would not pass a `0` index, since
Python sees that as `False`.

``` python
In [1]: ng = Ngrams(n_list=[1])

In [2]: ng.fit('text about stuff')

In [3]: ng.transform('text about stuff')
Out[3]: [1, 1, 1]

In [4]: ng.fit('n-grams are handy')

In [5]: ng.transform('text about n-grams')
Out[5]: [1, 1, 0, 1, 0, 0]

In [6]: ng.indices
Out[6]:
{('about',): 1,
 ('are',): 4,
 ('handy',): 5,
 ('n-grams',): 3,
 ('stuff',): 2,
 ('text',): 0}

```

The problem with this solution in terms of complexity is that it requires two
*full* pass-overs on whatever corpora you are feeding the model. One to
extract all possible combination, and another to construct vectors. It would
be a lot better if we could construct a vector or even matrix from the
frequencies that we can already extract at the `fit` step. For this, a
feature hasher such as the one [implemented in sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html), or the [dict vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer) would be
of great use, as will become clear after the next section.

## Method 2 - Sentences to Sparse Vectors

If some refactoring is done on the previous class, we could come up with the
following, very minimal function:

``` python
from collections import Counter

def extract_grams(sentence, n_list):
    inp = sentence.split()
    return Counter([gram for x in zip(*[inp[i:]
                    for i in range(n)]) for n in n_list])
```

This does exactly what we want:

``` python
In [1]: extract_grams('this is some text about text this is', n_list=[1, 2])
Out[1]: Counter({('about',): 1,
         ('about', 'text'): 1,
         ('is',): 2,
         ('is', 'some'): 1,
         ('some',): 1,
         ('some', 'text'): 1,
         ('text',): 2,
         ('text', 'about'): 1,
         ('text', 'this'): 1,
         ('this',): 2,
         ('this', 'is'): 2})
```
We can feed this to `FeatureHasher` so that it is transformed to a
[sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html) space. This
approach is clever in a number of ways: (1) $n$-grams can be extracted
iteratively and therefore your corpus does not need to be in memory. One could
for example read from a `.csv` and fit the hasher like so:

``` python
import csv
from sklearn.feature_extraction import FeatureHasher
from collections import Counter

def extract_grams(sentence, n_list):
    inp = sentence.split()
    return Counter([' '.join(gram) for x in zip(*[inp[i:]
                    for i in range(n)]) for n in n_list])

reader, hasher = csv.reader(open('some.csv', 'r')), FeatureHasher()
X = hasher.fit_transform([extract_grams(sentence, n_list=[1, 2])
                          for sentence in reader])
```
Note that the `' '.join` part was added because the `FeatureHasher` only
handles strings as features (`DictVectorizer` can deal with tuples though). So
there you have it, the most minimal way to quickly and memory efficiently get
a bag-of-words space.

Updates might follow.
