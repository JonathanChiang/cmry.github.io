<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1" /><title>(Pretty) Fast $n$-Gram Features with Python</title><meta name="twitter:card" content="summary" /><meta name="twitter:site" content="@_cmry" /><meta name="twitter:title" content="(Pretty) Fast $n$-Gram Features with Python" /><meta name="twitter:description" content="Recently I have been experimenting with (pretty) fast $n$-gram extraction for feature space construction. As I have clearly experienced, there are a bunch of caveats while building your own functio..."><meta name="description" content="Recently I have been experimenting with (pretty) fast $n$-gram extraction for feature space construction. As I have clearly experienced, there are a bunch of..."><link rel="icon" href="/assets/favicon.png"><link rel="apple-touch-icon" href="/assets/touch-icon.png"> <script type="text/x-mathjax-config"> MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"], ['\\(','\\)']]}});</script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" href="//code.cdn.mozilla.net/fonts/fira.css"><link rel="stylesheet" href="/assets/core.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="canonical" href="/notes/ngrams"><link rel="alternate" type="application/atom+xml" title="Chris Emmery" href="/feed.xml" /></head><body><aside class="logo"><nav> <a href="https://cmry.github.io"><i class="fa fa-home"></i></a> · <a href="https://www.github.com/cmry"><i class="fa fa-github"></i></a> · <a href="https://www.twitter.com/_cmry"><i class="fa fa-twitter"></i></a> · <a id="ind-publ" href="/publications">publ</a> · <a id="ind-code" href="/code">code</a> · <a id="ind-work" href="/work">work</a> · <a id="ind-about" href="/about">about</a></nav></aside><main> <noscript><style> article .footnotes { display: block; }</style></noscript><article><div class="center"><h1>(Pretty) Fast $n$-Gram Features with Python</h1><time>December 2, 2015 <br> <small> minute read</small></time></div><div class="divider"></div><p>Recently I have been experimenting with (pretty) fast $n$-gram extraction for feature space construction. As I have clearly experienced, there are a bunch of caveats while building your own functions. This blog post gives a <em>very</em> short introduction to $n$-grams, explains their extraction in examples as well as in code (section 2). And finally gives a very effective and low memory footprint method of extracting them (section 3).</p><h2 id="extracting-n-grams">Extracting $n$-Grams</h2><p>The extraction of $n$-grams is a commonly used as a so-called bag-of-words method used in <a href="https://en.wikipedia.org/wiki/Natural_language_processing">Natural Language Processing</a> (NLP) to represent a collection of documents. In a very basic example we want to either distinguish or relate sentence $A$, $B$, and $C$ which are the following:</p><div class="highlighter-rouge"><pre class="highlight"><code><span class="n">A</span> <span class="o">=</span> <span class="s">'text about stuff'</span>
<span class="n">B</span> <span class="o">=</span> <span class="s">'stuff about text'</span>
<span class="n">C</span> <span class="o">=</span> <span class="s">'text about n-grams'</span>
</code></pre></div><p>The $n$ in $n$-grams typically refers to a <em>scope</em>. One can look at it as going over the sentence with a window of some size. So with a 1-gram (referred to as a unigram) we partition the text in chunks of size one. So sentence $A$ would give us <code class="highlighter-rouge">text</code>, <code class="highlighter-rouge">about</code>, and <code class="highlighter-rouge">stuff</code>. If we would put this into a matrix of gram * sentence, we would get the following for the three sentences:</p><table><thead><tr><th> </th><th style="text-align: center">$A$</th><th style="text-align: center">$B$</th><th style="text-align: center">$C$</th></tr></thead><tbody><tr><td>text</td><td style="text-align: center">1</td><td style="text-align: center">1</td><td style="text-align: center">1</td></tr><tr><td>about</td><td style="text-align: center">1</td><td style="text-align: center">1</td><td style="text-align: center">1</td></tr><tr><td>stuff</td><td style="text-align: center">1</td><td style="text-align: center">1</td><td style="text-align: center">0</td></tr><tr><td>n-grams</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td></tr></tbody></table><p>Looking at the matrix, one can quickly see that despite the <em>order</em> of words, sentence $A$ and $B$ look the same. If we take their vector they can be observed to be identical:</p><p>$\vec{A} = [1, 1, 1, 0], \vec{B} = [1, 1, 1, 0]$</p><p>This is is what it means to have a bag-of-words. Words are thrown into a bag, scrambled in any order and counted. Order does not matter to the model. So how can we incorporate some structure into it? Taking word bi-grams, where $n = 2$ would probably already help our case. Consider:</p><table><thead><tr><th> </th><th style="text-align: center">$A$</th><th style="text-align: center">$B$</th><th style="text-align: center">$C$</th></tr></thead><tbody><tr><td>text about</td><td style="text-align: center">1</td><td style="text-align: center">0</td><td style="text-align: center">1</td></tr><tr><td>about stuff</td><td style="text-align: center">1</td><td style="text-align: center">0</td><td style="text-align: center">0</td></tr><tr><td>stuff about</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">0</td></tr><tr><td>about text</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">0</td></tr><tr><td>about n-grams</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td></tr></tbody></table><p>Now the vectors of $A$, $B$ and $C$ are all unique. Moreover, it can actually be inferred that $A$ and $C$ have some relation, but they both don’t relate to $B$. Done right? Well, not quite. What if we introduce a fourth sentence:</p><div class="highlighter-rouge"><pre class="highlight"><code><span class="n">D</span> <span class="o">=</span> <span class="s">'n-grams are handy'</span>
</code></pre></div><p>We construct the matrix again:</p><table><thead><tr><th> </th><th style="text-align: center">$A$</th><th style="text-align: center">$B$</th><th style="text-align: center">$C$</th><th style="text-align: center">$D$</th></tr></thead><tbody><tr><td>text about</td><td style="text-align: center">1</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">0</td></tr><tr><td>about stuff</td><td style="text-align: center">1</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">0</td></tr><tr><td>stuff about</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">0</td><td style="text-align: center">0</td></tr><tr><td>about text</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">0</td><td style="text-align: center">0</td></tr><tr><td>about n-grams</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">0</td></tr><tr><td>n-grams are</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td></tr><tr><td>are handy</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td></tr></tbody></table><p>We could say that $D$ relates to $C$, but it doesn’t show with this method. With our uni-gram method it might, but we run into trouble with $A$ and $B$ being identical again. Instead of words, we might draw more information from the character combinations instead. Let’s take tri-grams (3-grams), and see:</p><table><thead><tr><th> </th><th style="text-align: center">$A$</th><th style="text-align: center">$B$</th><th style="text-align: center">$C$</th><th style="text-align: center">$D$</th></tr></thead><tbody><tr><td>tex</td><td style="text-align: center">1</td><td style="text-align: center">1</td><td style="text-align: center">1</td><td style="text-align: center">0</td></tr><tr><td>ext</td><td style="text-align: center">1</td><td style="text-align: center">1</td><td style="text-align: center">1</td><td style="text-align: center">0</td></tr><tr><td>xta</td><td style="text-align: center">1</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">0</td></tr><tr><td>tab</td><td style="text-align: center">1</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">0</td></tr><tr><td>abo</td><td style="text-align: center">1</td><td style="text-align: center">1</td><td style="text-align: center">1</td><td style="text-align: center">0</td></tr><tr><td>bou</td><td style="text-align: center">1</td><td style="text-align: center">1</td><td style="text-align: center">1</td><td style="text-align: center">0</td></tr><tr><td>out</td><td style="text-align: center">1</td><td style="text-align: center">1</td><td style="text-align: center">1</td><td style="text-align: center">0</td></tr><tr><td>tst</td><td style="text-align: center">1</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">0</td></tr><tr><td>uts</td><td style="text-align: center">1</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">0</td></tr><tr><td>stu</td><td style="text-align: center">1</td><td style="text-align: center">1</td><td style="text-align: center">0</td><td style="text-align: center">0</td></tr><tr><td>tuf</td><td style="text-align: center">1</td><td style="text-align: center">1</td><td style="text-align: center">0</td><td style="text-align: center">0</td></tr><tr><td>uff</td><td style="text-align: center">1</td><td style="text-align: center">1</td><td style="text-align: center">0</td><td style="text-align: center">0</td></tr><tr><td>ffa</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">0</td><td style="text-align: center">0</td></tr><tr><td>fab</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">0</td><td style="text-align: center">0</td></tr><tr><td>utt</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">0</td><td style="text-align: center">0</td></tr><tr><td>tte</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">0</td><td style="text-align: center">0</td></tr><tr><td>utn</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">0</td></tr><tr><td>tn-</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">0</td></tr><tr><td>n-g</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">1</td></tr><tr><td>-gr</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">1</td></tr><tr><td>gra</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">1</td></tr><tr><td>ram</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">1</td></tr><tr><td>ams</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td><td style="text-align: center">1</td></tr><tr><td>msa</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td></tr><tr><td>sar</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td></tr><tr><td>are</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td></tr><tr><td>reh</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td></tr><tr><td>eha</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td></tr><tr><td>han</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td></tr><tr><td>and</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td></tr><tr><td>ndy</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">0</td><td style="text-align: center">1</td></tr></tbody></table><blockquote><p>Note that I removed spaces to avoid the matrix blowing up even more.</p></blockquote><p>Now, the overlap between the different sentences is more evident, but with an explosion of the matrix as a result. As can be observed, different grams vary in usefulness; for sentence relatedness, tri-grams might more useful - however, they obfuscate the actual word usage. The latter can be seen as problematic when the task is to distinguish certain topics. For example, if we want to measure the relatedness of sentences $A$ through $D$ to NLP, it’s more effective to represent <code class="highlighter-rouge">n-grams</code> as a full chunk rather than <code class="highlighter-rouge">['n-g', '-gr', 'gra', etc.]</code>. The explosion of the amount of grams per sentence makes it harder for algorithms to uncover relations; as it would have to rely on a combination of the grams for information, rather than their single occurrences. Another caveat is the sparseness of the vectors: the rarer the occurrences, the more zeroes are present in sentence vectors, the less information a vector provides per bit. In the above example this is already pretty evident from a very small example, imagine a corpus of a million documents. How one would go abouts programming this in an effective manner is the topic of this particular post.</p><h2 id="method-1---a-little-naive-but-dependency-free">Method 1 - a Little Naive, But Dependency Free</h2><p>First we will consider implementing a simple sliding window of $n$ to extract the grams from a sentence. Such that, given sentence $A$ again, we get the grams as we saw in the previous example. So something like:</p><div class="highlighter-rouge"><pre class="highlight"><code><span class="n">In</span> <span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">find_ngrams</span><span class="p">(</span><span class="s">'text about stuff'</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="p">[</span><span class="s">'text about'</span><span class="p">,</span> <span class="s">'about stuff'</span><span class="p">]</span>
</code></pre></div><p>Or even:</p><div class="highlighter-rouge"><pre class="highlight"><code><span class="n">In</span> <span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">find_ngrams</span><span class="p">(</span><span class="s">'text about stuff'</span><span class="p">,</span> <span class="n">n_list</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="p">[</span><span class="s">'text'</span><span class="p">,</span> <span class="s">'about'</span><span class="p">,</span> <span class="s">'stuff'</span><span class="p">,</span> <span class="s">'text about'</span><span class="p">,</span> <span class="s">'about stuff'</span><span class="p">]</span>
</code></pre></div><p>An effective algorithm as proven by <a href="https://stackoverflow.com/questions/21883108/fast-optimize-n-gram-implementations-in-python/21988533#21988533">this post</a> is to abuse zip and slicing in python. So:</p><div class="highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">find_ngrams</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="s">"""Magic n-gram function."""</span>
    <span class="n">inp</span> <span class="o">=</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="k">return</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">inp</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>

</code></pre></div><p>The above method will give us a list of tuples of size $n$, extracted in a sliding window. If we alter the function slightly, we will be able to achieve the desired result:</p><div class="highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">find_ngrams</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">n_list</span><span class="p">):</span>
    <span class="s">"""Magic n-gram function."""</span>
    <span class="n">inp</span><span class="p">,</span> <span class="n">grams</span> <span class="o">=</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">n_list</span><span class="p">:</span>
      <span class="n">grams</span> <span class="o">+=</span> <span class="p">[</span><span class="s">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">inp</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])]</span>
    <span class="k">return</span> <span class="n">grams</span>

</code></pre></div><p>Now we should be able to turn sentences into vectors representing the gram occurrences in a sentence. So that <code class="highlighter-rouge">the the a a thing</code> would at least yield <code class="highlighter-rouge">[2, 2, 1]</code>. This entails incorporating the search function into a neat class that can fit the known grams and make sure their index in the vector is the same for all sentences. A very compact class doing exactly this would look something like:</p><div class="highlighter-rouge"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Ngrams</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_list</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_list</span> <span class="o">=</span> <span class="n">n_list</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
        <span class="s">"""Magic n-gram function fits to vector indices."""</span>
        <span class="n">i</span><span class="p">,</span> <span class="n">inp</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_list</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">inp</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">x</span><span class="p">:</span> <span class="n">i</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
        <span class="s">"""Given a sentence, convert to a gram vector."""</span>
        <span class="n">v</span><span class="p">,</span> <span class="n">inp</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">),</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_list</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">inp</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">v</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">[</span><span class="n">x</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">v</span>
</code></pre></div><p>First we call the <code class="highlighter-rouge">fit</code> method to extract the n-grams and index them to <code class="highlighter-rouge">self.indices</code>. Next time the function sees a sentences, it will know where in a vector to place the frequencies, as well as which words are <em>not</em> part of that vector. This can be seen in the <code class="highlighter-rouge">transform</code> part where it begins with an empty vector of size <code class="highlighter-rouge">self.indices</code> and starts filling in the frequencies. A major drawback of this approach is that the corpus needs to be iterated over twice; once for extracting all possible $n$-grams, and once this is known, another passover to convert all sentences to vectors. Still, this works, proven by the example below. Please pay close attention to the <code class="highlighter-rouge">dict.get == None</code> or <code class="highlighter-rouge">!= None</code> parts. Given that we have a <code class="highlighter-rouge"><span class="p">{</span><span class="err">gram:</span><span class="w"> </span><span class="err">index</span><span class="p">}</span></code> dictionary, a simple <code class="highlighter-rouge">if self.indices.get</code> would not pass a <code class="highlighter-rouge">0</code> index, since Python sees that as <code class="highlighter-rouge">False</code>.</p><div class="highlighter-rouge"><pre class="highlight"><code><span class="n">In</span> <span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">ng</span> <span class="o">=</span> <span class="n">Ngrams</span><span class="p">(</span><span class="n">n_list</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">2</span><span class="p">]:</span> <span class="n">ng</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="s">'text about stuff'</span><span class="p">)</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">3</span><span class="p">]:</span> <span class="n">ng</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="s">'text about stuff'</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">3</span><span class="p">]:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">4</span><span class="p">]:</span> <span class="n">ng</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="s">'n-grams are handy'</span><span class="p">)</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">5</span><span class="p">]:</span> <span class="n">ng</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="s">'text about n-grams'</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">5</span><span class="p">]:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">6</span><span class="p">]:</span> <span class="n">ng</span><span class="o">.</span><span class="n">indices</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">6</span><span class="p">]:</span>
<span class="p">{(</span><span class="s">'about'</span><span class="p">,):</span> <span class="mi">1</span><span class="p">,</span>
 <span class="p">(</span><span class="s">'are'</span><span class="p">,):</span> <span class="mi">4</span><span class="p">,</span>
 <span class="p">(</span><span class="s">'handy'</span><span class="p">,):</span> <span class="mi">5</span><span class="p">,</span>
 <span class="p">(</span><span class="s">'n-grams'</span><span class="p">,):</span> <span class="mi">3</span><span class="p">,</span>
 <span class="p">(</span><span class="s">'stuff'</span><span class="p">,):</span> <span class="mi">2</span><span class="p">,</span>
 <span class="p">(</span><span class="s">'text'</span><span class="p">,):</span> <span class="mi">0</span><span class="p">}</span>

</code></pre></div><p>To counter the complexity issue from passing over whatever copora twice, it would be a lot better if a vector or even a matrix could be constructed from the frequencies that we can already extract at the <code class="highlighter-rouge">fit</code> step. For this, a feature hasher such as the one <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html">implemented in sklearn</a>, or the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer">dict vectorizer</a> would be of great use, as will become clear after the next section.</p><blockquote><p><a href="http://scikit-learn.org/stable/">Scikit-learn</a> is a very extensive toolkit for machine learning in Python. In addition to classifiers, it also provides tools for feature extraction, evaluation, optimization, etc. The FeatureHasher uses what is known as the hashing trick. If you’re interested in its workings, it is very well explained in <a href="http://blog.someben.com/2013/01/hashing-lang/">this</a> blog.</p></blockquote><h2 id="method-2---sentences-to-sparse-vectors">Method 2 - Sentences to Sparse Vectors</h2><p>If some refactoring is done on the previous class, we could come up with the following, very minimal function:</p><div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="k">def</span> <span class="nf">extract_grams</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">n_list</span><span class="p">):</span>
    <span class="n">inp</span> <span class="o">=</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">Counter</span><span class="p">([</span><span class="s">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">gram</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">inp</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">n_list</span><span class="p">])</span>
</code></pre></div><p>This does exactly what we want:</p><div class="highlighter-rouge"><pre class="highlight"><code><span class="n">In</span> <span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">extract_grams</span><span class="p">(</span><span class="s">'this is some text about text this is'</span><span class="p">,</span> <span class="n">n_list</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="n">Counter</span><span class="p">({(</span><span class="s">'about'</span><span class="p">,):</span> <span class="mi">1</span><span class="p">,</span>
         <span class="p">(</span><span class="s">'about'</span><span class="p">,</span> <span class="s">'text'</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span>
         <span class="p">(</span><span class="s">'is'</span><span class="p">,):</span> <span class="mi">2</span><span class="p">,</span>
         <span class="p">(</span><span class="s">'is'</span><span class="p">,</span> <span class="s">'some'</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span>
         <span class="p">(</span><span class="s">'some'</span><span class="p">,):</span> <span class="mi">1</span><span class="p">,</span>
         <span class="p">(</span><span class="s">'some'</span><span class="p">,</span> <span class="s">'text'</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span>
         <span class="p">(</span><span class="s">'text'</span><span class="p">,):</span> <span class="mi">2</span><span class="p">,</span>
         <span class="p">(</span><span class="s">'text'</span><span class="p">,</span> <span class="s">'about'</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span>
         <span class="p">(</span><span class="s">'text'</span><span class="p">,</span> <span class="s">'this'</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span>
         <span class="p">(</span><span class="s">'this'</span><span class="p">,):</span> <span class="mi">2</span><span class="p">,</span>
         <span class="p">(</span><span class="s">'this'</span><span class="p">,</span> <span class="s">'is'</span><span class="p">):</span> <span class="mi">2</span><span class="p">})</span>
</code></pre></div><p>We can feed this to the <code class="highlighter-rouge">FeatureHasher</code> so that it is transformed to a <a href="https://docs.scipy.org/doc/scipy/reference/sparse.html">sparse</a> space. This approach is clever in a number of ways: (1) $n$-grams can be extracted iteratively and therefore your corpus does not need to be in memory. One could for example read from a <code class="highlighter-rouge">.csv</code> and fit the hasher like so:</p><div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="kn">import</span> <span class="n">FeatureHasher</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="k">def</span> <span class="nf">extract_grams</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">n_list</span><span class="p">):</span>
    <span class="n">inp</span> <span class="o">=</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">Counter</span><span class="p">([</span><span class="s">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">gram</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">inp</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">n_list</span><span class="p">])</span>

<span class="n">reader</span><span class="p">,</span> <span class="n">hasher</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">reader</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s">'some.csv'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)),</span> <span class="n">FeatureHasher</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">hasher</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="n">extract_grams</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">n_list</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
                          <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">reader</span><span class="p">])</span>
</code></pre></div><p>Note that the <code class="highlighter-rouge">' '.join</code> is necessary because <code class="highlighter-rouge">FeatureHasher</code> only handles strings as features (<code class="highlighter-rouge">DictVectorizer</code> can deal with tuples though). (2) the iteration only has to be done once; therefore, you corpus does not have to be stored beforehand. In this sense, it can (3) be applied for fast online learning purposes seen in for example <a href="http://hunch.net/~vw/">Vowpal Wabbit</a> and <a href="http://torch.ch/">Torch</a>. So there you have it, a self-extensible, pretty minimal way to quickly and memory efficiently get a bag-of-words space.</p></article><div class="back"> <a href="/"></a></div></main></body></html>