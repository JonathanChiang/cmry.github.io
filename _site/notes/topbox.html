<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1" /><title>Topbox - Wrapping Stanford's Topic Modelling Toolbox for Classification Purposes</title><meta name="twitter:card" content="summary" /><meta name="twitter:site" content="@_cmry" /><meta name="twitter:title" content="Topbox - Wrapping Stanford's Topic Modelling Toolbox for Classification Purposes" /><meta name="twitter:description" content="For the line of research I started with my master thesis I made very frequent use of the Stanford Topic Modeling Toolbox (STMT). It is a very nifty module aimed to provide an interface to the Topic..."><meta name="description" content="For the line of research I started with my master thesis I made very frequent use of the Stanford Topic Modeling Toolbox (STMT). It is a very nifty module ai..."><link rel="icon" href="/assets/favicon.png"><link rel="apple-touch-icon" href="/assets/touch-icon.png"> <script type="text/x-mathjax-config"> MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"], ['\\(','\\)']]}});</script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro' rel='stylesheet' type='text/css'><link rel="stylesheet" href="/assets/core.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="canonical" href="/notes/topbox"><link rel="alternate" type="application/atom+xml" title="Chris Emmery" href="/feed.xml" /></head><body><aside class="logo"><nav><div class="links"> <a href="/"><i class="fa fa-home"></i></a> » <a id="ind-publ" href="/publ">publ</a> » <a id="ind-code" href="/code">code</a> » <a id="ind-work" href="/work">work</a> » <a id="ind-about" href="/about">about</a></div></nav></aside><main> <noscript><style> article .footnotes { display: block; }</style></noscript><article><h1>Topbox - Wrapping Stanford's Topic Modelling Toolbox for Classification Purposes</h1><time>June 18, 2015 | 7 minute read </time><div class="divider"></div><p>For the line of research I started with <a href="http://www.clips.ua.ac.be/sites/default/files/thesisfinal_p.pdf">my master thesis</a> I made <em>very</em> frequent use of the <a href="http://nlp.stanford.edu/software/tmt/tmt-0.4/">Stanford Topic Modeling Toolbox</a> (STMT). It is a very nifty module aimed to provide an interface to the <a href="https://en.wikipedia.org/wiki/Topic_model">Topic Models</a> by <a href="http://www.cs.columbia.edu/~blei/">David Blei</a>: <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/BleiNJ03.pdf">Latent Dirichlet Allocation</a> and its <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.184&amp;rep=rep1&amp;type=pdf">Supervised</a> variant. This toolbox was intended to make working with these models more accessible to researchers in the humanities and social sciences. It is therefore a standalone model that uses <code class="highlighter-rouge">.csv</code> files structured in the correct format as input, and <code class="highlighter-rouge">.scala</code> files for setting up the train - test routine. These can then be piped to the <code class="highlighter-rouge">.java</code> bundle, which will again output a directory with a trained model and several <code class="highlighter-rouge">.csv</code>s with its output. Pretty good stuff given that you do <strong>not</strong> want to use it for classification. The funny thing is, to date this toolbox is the only standalone implementation of Labelled-LDA, and it does not offer an intuitive interface for extracting results from a tested model. I figured, if one would require this, might as well just omit all the file hassle and make a nice script out of it.</p><h2 id="uh-what">Uh… what?</h2><p>Topic Modelling (or Modeling in US spelling) is a Machine Learning and Natural Language Processing task. Given a big set of texts (for example Wikipedia articles), a Topic Model tries to automatically find the topics in these articles. Say that we give it articles on mathematics, music, and sports - we would hope that it at least captures these three topics. It might even be able to get a more fine-grained set of topics such as algebra, geometry, concerts, instruments, football, and rugby. So in an unsupervised setting, we might tell the model that we think we have about 20 topics in our entire set. It will then figure out what words characterize these 20 individual topics, how probable it is that these words occur under this topic, and how probable the topic occurrence is in the given set. A Topic Model thus sees a topic as a distribution over words, like so:</p><div class="highlighter-rouge"><pre class="highlight"><code>topic 12, p = 0.418502
</code></pre></div><table><thead><tr><th style="text-align: center">Word</th><th style="text-align: center">p</th></tr></thead><tbody><tr><td style="text-align: center">soccer</td><td style="text-align: center">0.300</td></tr><tr><td style="text-align: center">rugby</td><td style="text-align: center">0.300</td></tr><tr><td style="text-align: center">goal</td><td style="text-align: center">0.200</td></tr><tr><td style="text-align: center">foul</td><td style="text-align: center">0.100</td></tr><tr><td style="text-align: center">score</td><td style="text-align: center">0.100</td></tr></tbody></table><p>Why is there ‘topic 12’ above a word distribution that we clearly can relate to sports? Well, we trained the model unsupervised; we didn’t tell it anything, just to extract 20 topics. It doesn’t know squat about what we see as topics and what topic names belong to which texts - it just allocates an arbitrary number. So what <strong>does</strong> it do then?</p><p>The intuition is that the model tries to find frequently co-occurring words, as they are argued to be likely to belong to the same topic. It also tries to find them unique to a certain topic; the words ‘it’ and ‘will’ are therefore likely to be deemed uninteresting throughout any of the topics. Topic Models are seen as generative models; by capturing how documents on certain topics could be written, one might combine topic distributions on Tv and Sports to generate a text talking about a sport commentary show. Well, in theory, that is - they don’t <em>really</em> write documents like a human would.</p><h2 id="topic-identification">Topic Identification</h2><p>When a Topic Model is trained supervised, the model has knowledge of what the documents you feed it are about. Typically, the input would have a (label, text) pair, like so:</p><div class="highlighter-rouge"><pre class="highlight"><code>sports, science     sEMG signal is a one dimensional time series signal of
                    neuromuscular system recorded from skin surface...
</code></pre></div><p>Now the model knows to fit the words given in our document under these two topic labels. What this implies is that when it is presented with <em>new</em> data, it can actually give a list with probabilities per topic. So say we already have this trained model lying around, we would see a result such as:</p><div class="highlighter-rouge"><pre class="highlight"><code>1: sports,  0.381
2: science, 0.212
3: health,  0.196
4: ...
</code></pre></div><p>The great thing about this, is that one can evaluate the performance of a model with common ranking measurements such as <a href="https://www.kaggle.com/wiki/MeanAveragePrecision">Mean Average Precision</a>. Provide the ranked set of labels, and compare it with the gold standard label(s) you already have, and it should return a score. In our case, we did pretty well and would score a $100\%$ on this particular instance (we even got the order right). So, we know how well our model is doing - and given that it performs nicely we can use it to classify new documents with one of the labels from our set.</p><blockquote><p><strong>Note:</strong> If you want a more in-depth introduction on these models, Chapter 3 in <a href="http://www.clips.ua.ac.be/sites/default/files/thesisfinal_p.pdf">my master thesis</a> (p. 17-33) explains it in more detail (bit of shameless self-promotion there).</p></blockquote><h2 id="topbox">topbox</h2><p>The process described above can all be done, or is made easier with <code class="highlighter-rouge">topbox</code>; a Python (2 &amp; 3) wrapper around STMT. So let’s dig into its functionality; we already know what needs doing. We need a nice small train and test set and we should be good to go. As such:</p><div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">train</span> <span class="o">=</span> <span class="p">[[</span><span class="s">'sports football'</span><span class="p">,</span> <span class="s">'about football, soccer, with a goal and a ball'</span><span class="p">],</span>
         <span class="p">[</span><span class="s">'sports rugby'</span><span class="p">,</span> <span class="s">'some text where we do a scrum and kick the ball'</span><span class="p">],</span>
         <span class="p">[</span><span class="s">'music concerts'</span><span class="p">,</span> <span class="s">'a venue with loud music and a stage'</span><span class="p">],</span>
         <span class="p">[</span><span class="s">'music instruments'</span><span class="p">,</span> <span class="s">'thing that have strings or keys, or whatever'</span><span class="p">]]</span>

<span class="n">test</span> <span class="o">=</span> <span class="p">[[</span><span class="s">'music'</span><span class="p">,</span> <span class="s">'the stage was full of string things'</span><span class="p">],</span>
        <span class="p">[</span><span class="s">'sports'</span><span class="p">,</span> <span class="s">'we kick a ball around'</span><span class="p">],</span>
        <span class="p">[</span><span class="s">'rugby'</span><span class="p">,</span> <span class="s">'some confusing sentence with novel words what is happening'</span><span class="p">]]</span>
</code></pre></div><p>With the example above things get a bit convoluted, because we actually need a split list with documents and a separate list for the labels. We can do that anyway, let’s first import and call the <code class="highlighter-rouge">topbox</code> environment:</p><div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">topbox</span>

<span class="n">stmt</span> <span class="o">=</span> <span class="n">topbox</span><span class="o">.</span><span class="n">STMT</span><span class="p">(</span><span class="s">'test_model'</span><span class="p">)</span>
</code></pre></div><p>The module is going to store whatever model we train in <code class="highlighter-rouge">/directory/to/topbox/box/</code>, under the name ‘test_model’. If you want to train and call it later, you can, but please be minded that it will consume disk space to keep the model stored in the box. We called <code class="highlighter-rouge">STMT</code> completely parameter-less now, however. If we look at the documentation we get an idea of what the options are:</p><h3 id="stmtparameters">STMT.Parameters</h3><div class="language-text highlighter-rouge"><pre class="highlight"><code>name : string
    The name that will be appended to all the saved files. If you want to
    keep the trained model, this name can be used to load it back in.

epochs : integer, optional, default 20
    The amount of iterations you want L-LDA to train and sample; if you
    run into some errors, it's a good idea to set this to 1 to save time
    whilst debugging.

mem : integer, optional, default 7000
    The amount of memory (in MB) that the model will use. By default it
    assumes that you have 8G of memory, so it will account for 1G of os
    running. Should be comfortable; adjust if running into OutOfMemory
    errors though.

keep : boolean, optional, default True
    If set to False, will remove the data and scala files after training,
    and will remove EVERYTHING after the resutls are obtained. This can
    be handy when running a quick topic model and save disk space. If
    you're running a big model and want to keep it after your session is
    done, it might be better to just leave it to True.
</code></pre></div><p>So if we want to allocate more memory, and do for example 400 iterations, we call:</p><div class="language-python highlighter-rouge"><pre class="highlight"><code>
<span class="n">stmt</span> <span class="o">=</span> <span class="n">topbox</span><span class="o">.</span><span class="n">STMT</span><span class="p">(</span><span class="s">'test_model'</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">mem</span><span class="o">=</span><span class="mi">14000</span><span class="p">)</span>

</code></pre></div><p>Now we need to do a quick unzip for both of our sets, lets split them up in labels and space respectively:</p><div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">train_labels</span><span class="p">,</span> <span class="n">train_space</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">train</span><span class="p">)</span>
<span class="n">test_labels</span><span class="p">,</span> <span class="n">test_space</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">test</span><span class="p">)</span>
</code></pre></div><p>Of course, if you already have these separately, no need to go through that hassle. So from here, it’s pretty straightforward, we just test and train:</p><div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">stmt</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_space</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
<span class="n">stmt</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">test_space</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>
</code></pre></div><p>If all is well, you should see your terminal call <code class="highlighter-rouge">java</code> and do iterations of training, as well as reading in the items for testing. After, we can make <code class="highlighter-rouge">topbox</code> retrieve the result. Now please note that we can either return the whole thing as a list (which might yield empty rows if a topic label to classify is not in your training set for example), which will keep the dependencies to the standard library of Python. Alternatively, you can make <code class="highlighter-rouge">topbox</code> convert the whole thing to an array, which will require both <code class="highlighter-rouge">numpy</code> and <code class="highlighter-rouge">scipy</code>. Call the results, and dump these into <code class="highlighter-rouge">y_true</code> and <code class="highlighter-rouge">y_score</code>, providing the correct reference labels. As such:</p><div class="language-python highlighter-rouge"><pre class="highlight"><code>
<span class="c"># list</span>
<span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span> <span class="o">=</span> <span class="n">stmt</span><span class="o">.</span><span class="n">results</span><span class="p">(</span><span class="n">test_labels</span><span class="p">)</span>

<span class="c"># array (sklearn ready)</span>
<span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span> <span class="o">=</span> <span class="n">stmt</span><span class="o">.</span><span class="n">results</span><span class="p">(</span><span class="n">test_labels</span><span class="p">,</span> <span class="n">array</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div><p>Given that we go for the latter option, we can immediately evaluate the results with some evaluation metric of choice from <code class="highlighter-rouge">sklearn</code>.</p><div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">In</span> <span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">average_precision_score</span>
        <span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span><span class="p">)</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span> <span class="mf">0.86888190259464704</span>
</code></pre></div><p>After, if we do not want to use the model any more, we can simply get rid of it by calling:</p><div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">stmt</span><span class="o">.</span><span class="n">cleanup</span><span class="p">()</span>
</code></pre></div><p>Forgot the name, want to get rid off al the models you trained?</p><div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">stmt</span><span class="o">.</span><span class="n">cleanup</span><span class="p">(</span><span class="nb">all</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div><h3 id="applying-trained-topbox">Applying trained topbox</h3><p>To recap, here’s an example that quickly trains a model and tests if it works on the same data (please never do this other than for debugging purposes, see below).</p><div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">csv</span>

<span class="o">%</span><span class="n">cd</span> <span class="o">~/</span><span class="n">Documents</span><span class="o">/</span><span class="n">data</span>

<span class="n">csv_reader</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">reader</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s">'train_data.csv'</span><span class="p">))</span>
<span class="c"># relevant cells are in the 5th and 7th column</span>
<span class="n">dat</span> <span class="o">=</span> <span class="p">[(</span><span class="n">x</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="n">x</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">csv_reader</span><span class="p">]</span>

<span class="o">%</span><span class="n">cd</span> <span class="o">~/</span><span class="n">Documents</span><span class="o">/</span>

<span class="kn">import</span> <span class="nn">topbox</span>

<span class="n">stmt</span> <span class="o">=</span> <span class="n">topbox</span><span class="o">.</span><span class="n">STMT</span><span class="p">(</span><span class="s">'bit_of_testing'</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">mem</span><span class="o">=</span><span class="mi">15000</span><span class="p">)</span>

<span class="n">train_labels</span><span class="p">,</span> <span class="n">train_space</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">dat</span><span class="p">)</span>
<span class="n">stmt</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_space</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
<span class="n">stmt</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">train_labels</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">average_precision_score</span>

<span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span> <span class="o">=</span> <span class="n">stmt</span><span class="o">.</span><span class="n">results</span><span class="p">(</span><span class="n">test_labels</span><span class="p">,</span> <span class="n">array</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span><span class="p">))</span>
</code></pre></div><p>Now the model we trained is actually very much overfitted; we can’t assess how it generalizes topics because we give it the exact data that it already knows. Let’s try to train it tenfold cross-validation setting, like so:</p><div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">average_precision_score</span>

<span class="n">k</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dat</span><span class="p">)</span><span class="o">/</span><span class="n">k</span><span class="p">)</span>
<span class="n">ap_tot</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">test</span><span class="p">),</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">stmt</span> <span class="o">=</span> <span class="n">topbox</span><span class="o">.</span><span class="n">STMT</span><span class="p">(</span><span class="s">'testing_cf_'</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">mem</span><span class="o">=</span><span class="mi">15000</span><span class="p">)</span>
    <span class="c"># split lists</span>
    <span class="n">train</span> <span class="o">=</span> <span class="n">dat</span><span class="p">[:]</span>
    <span class="n">test</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">n</span><span class="p">]</span>
    <span class="n">train</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c"># train / test</span>
    <span class="n">train_labels</span><span class="p">,</span> <span class="n">train_space</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">train</span><span class="p">)</span>
    <span class="n">stmt</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_space</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
    <span class="n">test_labels</span><span class="p">,</span> <span class="n">test_space</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">test</span><span class="p">)</span>
    <span class="n">stmt</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">test_space</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>
    <span class="c"># get scores</span>
    <span class="n">y_true_k</span><span class="p">,</span> <span class="n">y_score_k</span> <span class="o">=</span> <span class="n">stmt</span><span class="o">.</span><span class="n">results</span><span class="p">(</span><span class="n">test_labels</span><span class="p">,</span> <span class="n">array</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">ap_tot</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_true_k</span><span class="p">,</span> <span class="n">y_score_k</span><span class="p">))</span>
<span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ap_tot</span><span class="p">)</span>
</code></pre></div><p>Now we actually get average performance on new data. And we’re done! I will try to upload the package to Github as soon as possible, improve the documentation, and provide some toy dataset to test the stuff with. Update will follow.</p></article><div class="back"> <a href="/"></a></div></main></body></html>